{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Klax","text":"<p>A lightweight machine learning package for computational mechanics built on JAX.</p> <p>Warning</p> <p>Klax is still in early development and will likely see significant API changes in the near future. Likewise, the documentation is still under heavy development.</p>"},{"location":"#overview","title":"Overview","text":"<p>Klax provides specialized machine learning architectures, constraints, and training utilities for mechanics and physics applications. Built on top of JAX, Equinox, and Optax, it offers:</p> <ul> <li>Special Neural Networks: Implementations of, e.g.,  Input Convex Neural Networks (ICNNs), matrix-valued neural networks, MLPs with custom initialization, and more.</li> <li>JAX Compatibility: Seamless integration with JAX's automatic differentiation and acceleration</li> <li>Parameter Constraints: Differentiable and non-differentiable parameter constraints through <code>klax.Unwrappable</code> and <code>klax.Constraint</code></li> <li>Customizable Training: Methods and APIs for customized calibrations on arbitrary PyTree data structures through <code>klax.fit</code>, <code>klax.Loss</code>, and <code>klax.Callback</code>.</li> </ul> <p>Klax is designed to be minimally intrusive - all models inherit directly from <code>equinox.Module</code> without additional abstraction layers. This ensures full compatibility with the JAX/Equinox ecosystem.</p> <p>The constraint system is derived from Paramax's <code>paramax.AbstractUnwrappable</code>, extending it to support non-differentiable/zero-gradient parameter constraints such as ReLU-based non-negativity constraints.</p> <p>The provided calibration utilities (<code>klax.fit</code>, <code>klax.Loss</code>, <code>klax.Callback</code>) are designed to operate on arbitrarily shaped PyTrees of data, fully utilizing the flexibility of JAX and Equinox. While they cover most common machine learning use cases, as well as our specialized requirements, they remain entirely optional. The core building blocks of Klax work seamlessly in custom training loops.</p> <p>Currently Klax's training utilities are built around Optax, but different optimization libraries could be supported in the future if desired.</p>"},{"location":"#installation","title":"Installation","text":"<p>Klax can be installed via pip using</p> <pre><code>pip install klax\n</code></pre> <p>If you want to add the latest release of klax to your Python uv project run</p> <pre><code>uv add klax\n</code></pre> <p>or get the most recent changes from the main branch via</p> <pre><code>uv add \"klax @ git+https://github.com/Drenderer/klax.git@main\"\n</code></pre>"},{"location":"#getting-started","title":"Getting Started","text":"<p>If you're new to the JAX ecosystem, we recommend looking at Quickstart, which provides a concise overview of JAX's core functionality.</p> <p>As the contents of the documentation are still rudimentary, we recommend checking out Equinox and Paramax and taking a look at our examples.</p>"},{"location":"#citation","title":"Citation","text":""},{"location":"#acknowledgement","title":"Acknowledgement","text":"<p>Klax is built on top of several powerful frameworks:</p> <p>JAX - For automatic differentiation and acceleration  Equinox - For neural network primitives  Optax - For optimization utilities  Paramax - For constraints (We decided to embed Paramax directly into Klax due to the need for non-differentiable constraints).</p>"},{"location":"api/callbacks/","title":"Callbacks","text":""},{"location":"api/callbacks/#klax.CallbackArgs","title":"<code>klax.CallbackArgs</code> <code></code>","text":"<p>A callback argument designed to work in conjunction with <code>klax.fit</code>.</p> <p>This class should not be instantiated directly. An instance of this class is passed to every callback object in the fit function. When writing a custom callback, use the properties of this class to access the current model, optimizer state, training data, and validation data during training.</p> <p>This class implements cached and lazy-evaluated values via property methods. This means that properties like <code>loss</code> are only calculated if they are used and are stored such that they are not calculated multiple times.</p>"},{"location":"api/callbacks/#klax.CallbackArgs.loss","title":"<code>loss</code>  <code>property</code>","text":"<p>Lazy-evaluated and cached training loss.</p>"},{"location":"api/callbacks/#klax.CallbackArgs.model","title":"<code>model</code>  <code>property</code>","text":"<p>Lazy-evaluated and cached model.</p>"},{"location":"api/callbacks/#klax.CallbackArgs.opt_state","title":"<code>opt_state</code>  <code>property</code>","text":"<p>Lazy-evaluated and cached optimizer state.</p>"},{"location":"api/callbacks/#klax.CallbackArgs.val_loss","title":"<code>val_loss</code>  <code>property</code>","text":"<p>Lazy-evaluated and cached validation loss.</p>"},{"location":"api/callbacks/#klax.CallbackArgs.__init__","title":"<code>__init__(get_loss, treedef_model, treedef_opt_state, data, val_data=None)</code> <code></code>","text":"<p>Initialize the callback arguments object.</p> <p>get_loss: Function that takes a model and a batch of data and     returns the loss. treedef_model: Tree structure of the model. treedef_opt_state: Tree structure of the :py:mod:<code>optax</code> optimizer. data: PyTree of the training data. val_data: PyTree of the validation data. If None, no validation     loss is calculated and the property :py:attr:<code>val_loss</code> will     return None.</p>"},{"location":"api/callbacks/#klax.CallbackArgs.update","title":"<code>update(flat_model, flat_opt_state, step)</code> <code></code>","text":"<p>Update the object with the current model and optimizer state.</p> <p>This method is called repeatedly in <code>klax.fit</code>.</p> PARAMETER DESCRIPTION <code>flat_model</code> <p>Flattened PyTree of the model.</p> <p> TYPE: <code>PyTree</code> </p> <code>flat_opt_state</code> <p>Flattened PyTree of the <code>optax</code> optimizer.</p> <p> TYPE: <code>PyTree</code> </p> <code>step</code> <p>Current step-count of the training.</p> <p> TYPE: <code>int</code> </p>"},{"location":"api/callbacks/#klax.Callback","title":"<code>klax.Callback</code> <code></code>","text":"<p>An abstract callback.</p> <p>Inherit from this class to create a custom callback.</p>"},{"location":"api/callbacks/#klax.Callback.__call__","title":"<code>__call__(cbargs)</code> <code></code>","text":"<p>Call after each step during training.</p>"},{"location":"api/callbacks/#klax.Callback.on_training_start","title":"<code>on_training_start(cbargs)</code> <code></code>","text":"<p>Call when training starts.</p>"},{"location":"api/callbacks/#klax.Callback.on_training_end","title":"<code>on_training_end(cbargs)</code> <code></code>","text":"<p>Call when training ends.</p>"},{"location":"api/callbacks/#klax.HistoryCallback","title":"<code>klax.HistoryCallback(klax.Callback)</code> <code></code>","text":"<p>Default callback for logging a training process.</p> <p>Records loss histories, training time, and the last optimizer state.</p>"},{"location":"api/callbacks/#klax.HistoryCallback.__init__","title":"<code>__init__(log_every=100, verbose=True)</code> <code></code>","text":"<p>Initialize the <code>HistoryCallback</code>.</p> <p>log_every: Amount of steps after which the training and validation     losses are logged. (Defaults to 100.) verbose: If true prints the training progress and losses.     (Defaults to True.)</p>"},{"location":"api/callbacks/#klax.HistoryCallback.__call__","title":"<code>__call__(cbargs)</code> <code></code>","text":"<p>Record the losses and step count.</p> <p>Called at each step during training.</p>"},{"location":"api/callbacks/#klax.HistoryCallback.load","title":"<code>load(filename)</code> <code></code>  <code>staticmethod</code>","text":"<p>Load a <code>HistoryCallback</code> instance from a file.</p> PARAMETER DESCRIPTION <code>filename</code> <p>The file path from which the instance should be loaded.</p> <p> TYPE: <code>str | pathlib.Path</code> </p> RETURNS DESCRIPTION <code>klax.HistoryCallback</code> <p>The loaded <code>HistoryCallback</code> instance.</p> RAISES DESCRIPTION <code>ValueError</code> <p>If the file is not a valid pickle file or does not contain a <code>HistoryCallback</code> instance.</p>"},{"location":"api/callbacks/#klax.HistoryCallback.save","title":"<code>save(filename, overwrite=False, create_dir=True)</code> <code></code>","text":"<p>Save the HistoryCallback instance to a file using pickle.</p> PARAMETER DESCRIPTION <code>filename</code> <p>The file path where the instance should be saved.</p> <p> TYPE: <code>str | pathlib.Path</code> </p> <code>overwrite</code> <p>If True, overwrite the file if it already exists. If False, raise a FileExistsError if the file exists. (Defaults to False.)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>create_dir</code> <p>If True, create the parent directory if it does not exist. (Defaults to True.)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> RAISES DESCRIPTION <code>FileExistsError</code> <p>If the file already exists and overwrite is False.</p> <code>ValueError</code> <p>If the provided path is not a valid file path.</p>"},{"location":"api/callbacks/#klax.HistoryCallback.plot","title":"<code>plot(*, ax=None, loss_options={}, val_loss_options={})</code> <code></code>","text":"<p>Plot the recorded training and validation losses.</p> Note <p>This method requires matplotlib.</p> PARAMETER DESCRIPTION <code>ax</code> <p>Matplotlib axes to plot into. If <code>None</code> then a new axis is created. (Defaults to None.)</p> <p> TYPE: <code>Any</code> DEFAULT: <code>None</code> </p> <code>loss_options</code> <p>Dictionary of keyword arguments passed to matplotlibs <code>plot</code> for the training loss. (Defaults to {}.)</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> <code>val_loss_options</code> <p>Dictionary of keyword arguments passed to matplotlibs <code>plot</code> for the validation loss. (Defaults to {}.)</p> <p> TYPE: <code>dict</code> DEFAULT: <code>{}</code> </p> RAISES DESCRIPTION <code>ImportError</code> <p>description</p>"},{"location":"api/callbacks/#klax.HistoryCallback.on_training_start","title":"<code>on_training_start(cbargs)</code> <code></code>","text":"<p>Initialize the training start time.</p> <p>Called at beginning of training.</p>"},{"location":"api/callbacks/#klax.HistoryCallback.on_training_end","title":"<code>on_training_end(cbargs)</code> <code></code>","text":"<p>Record the training end time and the last optimizer state.</p> <p>Called at end of training.</p>"},{"location":"api/losses/","title":"Loss functions","text":""},{"location":"api/losses/#klax.Loss","title":"<code>klax.Loss</code> <code></code>","text":"<p>An abstract callable loss object.</p> <p>It can be used to build custom losses that can be passed to <code>klax.fit</code>.</p> Example <p>A simple custom loss that computes the mean squared error between the predicted values <code>y_pred</code> and true values <code>y</code> for in inputs <code>x</code> may be implemented as follows:</p> <pre><code>&gt;&gt;&gt; def mse(model, data, batch_axis=0):\n...    x, y = data\n...    if isinstance(batch_axis, tuple):\n...        in_axes = batch_axis[0]\n...    else:\n...        in_axes = batch_axis\n...    y_pred = jax.vmap(model, in_axes=(in_axes,))(x)\n...    return jnp.mean(jnp.square(y_pred - y))\n</code></pre> <p>Note that, since we a aim to provide a maximum of flexibility the users have to take care of applying <code>jax.vmap</code> to the model themselves.</p>"},{"location":"api/losses/#klax.Loss.__call__","title":"<code>__call__(model, data, batch_axis)</code> <code></code>","text":"<p>Abstract method to compute the loss for a given model and data.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model parameters or structure to evaluate the loss.</p> <p> TYPE: <code>PyTree</code> </p> <code>data</code> <p>The input data or structure used for loss computation.</p> <p> TYPE: <code>PyTree</code> </p> <code>batch_axis</code> <p>Specifies the axis or axes corresponding to the batch dimension in the data. Can be an integer, None, or a sequence of values.</p> <p> TYPE: <code>int | None | Sequence[Any]</code> </p> RETURNS DESCRIPTION <code>Scalar</code> <p>The computed loss value.</p> <p> TYPE: <code>Shaped[Array, '']</code> </p>"},{"location":"api/losses/#klax.MSE","title":"<code>klax.MSE(klax.Loss)</code> <code></code>","text":"<p>Mean squared error for a tuple of data <code>(x, y)</code>.</p> <p>The inputs <code>x</code> and the outputs <code>y</code> are expected to have the same batch axis and equal length along that axis.</p>"},{"location":"api/losses/#klax.MSE.__call__","title":"<code>__call__(model, data, batch_axis=0)</code> <code></code>","text":""},{"location":"api/losses/#klax.mse","title":"<code>klax.mse(model, data, batch_axis=0)</code>","text":"<p>Mean squared error for a tuple of data <code>(x, y)</code>.</p> <p>The inputs <code>x</code> and the outputs <code>y</code> are expected to have the same batch axis and equal length along that axis.</p>"},{"location":"api/losses/#klax.MAE","title":"<code>klax.MAE(klax.Loss)</code> <code></code>","text":"<p>Mean absolute error for a tuple of data <code>(x, y)</code>.</p> <p>The inputs <code>x</code> and the outputs <code>y</code> are expected to have the same batch axis and equal length along that axis.</p>"},{"location":"api/losses/#klax.MAE.__call__","title":"<code>__call__(model, data, batch_axis=0)</code> <code></code>","text":""},{"location":"api/losses/#klax.mae","title":"<code>klax.mae(model, data, batch_axis=0)</code>","text":"<p>Mean absolute error for a tuple of data <code>(x, y)</code>.</p> <p>The inputs <code>x</code> and the outputs <code>y</code> are expected to have the same batch axis and equal length along that axis.</p>"},{"location":"api/serialization/","title":"Serialization","text":""},{"location":"api/serialization/#klax.text_serialize_filter_spec","title":"<code>klax.text_serialize_filter_spec(f, x)</code> <code></code>","text":"<p>Filter specification for serializing a leaf to text.</p> PARAMETER DESCRIPTION <code>f</code> <p>File-like object to write to.</p> <p> TYPE: <code>BinaryIO</code> </p> <code>x</code> <p>The leaf to save in the file.</p> <p> TYPE: <code>Any</code> </p> Example <p>Serializing a model to a text file.</p> <pre><code>&gt;&gt;&gt; import equinox as eqx\n&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; import klax\n&gt;&gt;&gt;\n&gt;&gt;&gt; tree = (jnp.array([1, 2, 3]), [3, 4, 5])\n&gt;&gt;&gt; eqx.tree_serialize_leaves(\n...     \"some_txt_file.txt\",\n...     tree,\n...     filter_spec=klax.text_serialize_filter_spec\n... )\n</code></pre>"},{"location":"api/serialization/#klax.text_deserialize_filter_spec","title":"<code>klax.text_deserialize_filter_spec(f, x)</code> <code></code>","text":"<p>Filter specification for deserializing a leaf from text.</p> <p>This function can be used to deserialized leafs that have been serialized using <code>klax.text_serialize_filter_spec</code>.</p> PARAMETER DESCRIPTION <code>f</code> <p>File-like object to read from.</p> <p> TYPE: <code>BinaryIO</code> </p> <code>x</code> <p>The leaf for which to load data from the file.</p> <p> TYPE: <code>Any</code> </p> Example <pre><code>&gt;&gt;&gt; import equinox as eqx\n&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; import klax\n&gt;&gt;&gt;\n&gt;&gt;&gt; tree = (jnp.array([1, 2, 3]), [3, 4, 5])\n&gt;&gt;&gt; eqx.tree_serialize_leaves(\n...     \"some_txt_file.txt\",\n...     tree,\n...     filter_spec=klax.text_serialize_filter_spec\n... )\n&gt;&gt;&gt; loaded_tree = eqx.tree_deserialize_leaves(\n...     \"some_txt_file.txt\",\n...     tree,\n...     filter_spec=klax.text_deserialize_filter_spec\n... )\n</code></pre>"},{"location":"api/training/","title":"Calibration and Data Handling","text":""},{"location":"api/training/#klax.batch_data","title":"<code>klax.batch_data(data, batch_size=32, batch_axis=0, convert_to_numpy=True, *, key)</code> <code></code>","text":"<p>Create a <code>Generator</code> that draws subsets of data without replacement.</p> <p>The data can be any <code>PyTree</code> with <code>ArrayLike</code> leaves. If <code>batch_mask</code> is passed, batch axes (including <code>None</code> for no batching) can be specified for every leaf individualy. A generator is returned that indefinetly yields batches of data with size <code>batch_size</code>. Examples are drawn without replacement until the remaining dataset is smaller than <code>batch_size</code>, at which point the dataset will be reshuffeld and the process starts over.</p> Example <p>This is an example for a nested <code>PyTree</code>, where the elements x and y have batch dimension along the first axis.</p> <pre><code>&gt;&gt;&gt; import klax\n&gt;&gt;&gt; import jax\n&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt;\n&gt;&gt;&gt; x = jnp.array([1., 2.])\n&gt;&gt;&gt; y = jnp.array([[1.], [2.]])\n&gt;&gt;&gt; data = (x, {\"a\": 1.0, \"b\": y})\n&gt;&gt;&gt; batch_mask = (0, {\"a\": None, \"b\": 0})\n&gt;&gt;&gt; iter_data = klax.batch_data(\n...     data,\n...     32,\n...     batch_mask,\n...     key=jax.random.key(0)\n... )\n&gt;&gt;&gt;\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>The data that shall be batched. It can be any <code>PyTree</code> with <code>ArrayLike</code> leaves.</p> <p> TYPE: <code>PyTree[Any]</code> </p> <code>batch_size</code> <p>The number of examples in a batch.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>batch_axis</code> <p>PyTree of the batch axis indices. <code>None</code> is used to indicate that the corresponding leaf or subtree in data does not have a batch axis. <code>batch_axis</code> must have the same structure as <code>data</code> or have <code>data</code> as a prefix. (Defaults to 0, meaning all leaves in <code>data</code> are batched along their first dimension.)</p> <p> TYPE: <code>PyTree[None | int]</code> DEFAULT: <code>0</code> </p> <code>convert_to_numpy</code> <p>If <code>True</code>, batched data leafs will be converted to Numpy arrays before batching. This is useful for performance reasons, as Numpy's slicing is much faster than JAX's.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>key</code> <p>A <code>jax.random.PRNGKey</code> used to provide randomness for batch generation. (Keyword only argument.)</p> <p> TYPE: <code>PRNGKeyArray</code> </p> RETURNS DESCRIPTION <code>Generator[PyTree[Any], None, None]</code> <p>A <code>Generator</code> that yields a random batch of data every time is is</p> <code>Generator[PyTree[Any], None, None]</code> <p>called.</p> YIELDS DESCRIPTION <code>Generator[PyTree[Any], None, None]</code> <p>A <code>PyTree[ArrayLike]</code> with the same structure as <code>data</code>. Where all</p> <code>Generator[PyTree[Any], None, None]</code> <p>batched leaves have <code>batch_size</code>.</p> Note <p>Note that if the size of the dataset is smaller than <code>batch_size</code>, the obtained batches will have dataset size.</p>"},{"location":"api/training/#klax.split_data","title":"<code>klax.split_data(data, proportions, batch_axis=0, *, key)</code> <code></code>","text":"<p>Split a <code>PyTree</code> of data into multiply randomly drawn subsets.</p> <p>This function is useful for splitting into training and test datasets. The axis of the split if controlled by the <code>batch_axis</code> argument, which specifies the batch axis for each leaf in <code>data</code>.</p> Example <p>This is an example for a nested PyTree` of data.</p> <pre><code>&gt;&gt;&gt; import klax\n&gt;&gt;&gt; import jax\n&gt;&gt;&gt;\n&gt;&gt;&gt; x = jax.numpy.array([1., 2., 3.])\n&gt;&gt;&gt; data = (x, {\"a\": 1.0, \"b\": x})\n&gt;&gt;&gt; s1, s2 = klax.split_data(\n...     data,\n...     (2, 1),\n...     key=jax.random.key(0)\n... )\n&gt;&gt;&gt; s1\n(Array([1., 2.], dtype=float32), {'a': 1.0, 'b': Array([1., 2.], dtype=float32)})\n&gt;&gt;&gt; s2\n(Array([3.], dtype=float32), {'a': 1.0, 'b': Array([3.], dtype=float32)})\n</code></pre> PARAMETER DESCRIPTION <code>data</code> <p>Data that shall be split. It can be any <code>PyTree</code>.</p> <p> TYPE: <code>PyTree[Any]</code> </p> <code>proportions</code> <p>Proportions of the split that will be applied to the data, e.g., <code>(80, 20)</code> for a 80% to 20% split. The proportions must be non-negative.</p> <p> TYPE: <code>Sequence[int | float]</code> </p> <code>batch_axis</code> <p>PyTree of the batch axis indices. <code>None</code> is used to indicate that the corresponding leaf or subtree in data does not have a batch axis. <code>batch_axis</code> must have the same structure as <code>data</code> or have <code>data</code> as a prefix. (Defaults to 0)</p> <p> TYPE: <code>PyTree[None | int]</code> DEFAULT: <code>0</code> </p> <code>key</code> <p>A <code>jax.random.PRNGKey</code> used to provide randomness to the split. (Keyword only argument.)</p> <p> TYPE: <code>PRNGKeyArray</code> </p> RETURNS DESCRIPTION <code>tuple[PyTree[Any], ...]</code> <p>Tuple of <code>PyTrees</code>.</p>"},{"location":"api/training/#klax.fit","title":"<code>klax.fit(model, data, *, batch_size=32, batch_axis=0, validation_data=None, steps=1000, loss_fn=&lt;klax._losses.MSE object at 0x7feadbafb800&gt;, optimizer=GradientTransformationExtraArgs(init=&lt;function chain.&lt;locals&gt;.init_fn&gt;, update=&lt;function chain.&lt;locals&gt;.update_fn&gt;), init_opt_state=None, batcher=&lt;function batch_data&gt;, history=None, callbacks=None, key)</code> <code></code>","text":"<p>Trains a model using an optimizer from optax.</p> PARAMETER DESCRIPTION <code>model</code> <p>The model instance, which should be trained. It must be a subclass of <code>equinox.Module</code>. The model may contain <code>klax.Unwrappable</code> wrappers.</p> <p> TYPE: <code>T</code> </p> <code>data</code> <p>The training data can be any <code>PyTree</code> with <code>ArrayLike</code> leaves. Most likely you'll want <code>data</code> to be a tuple <code>(x, y)</code> with model inputs <code>x</code> and model outputs <code>y</code>.</p> <p> TYPE: <code>PyTree[Any]</code> </p> <code>batch_size</code> <p>The number of examples in a batch.</p> <p> TYPE: <code>int</code> DEFAULT: <code>32</code> </p> <code>batch_axis</code> <p>A <code>PyTree</code> denoting, which axis is the batch axis for arrays in <code>data</code>. <code>batch_axis</code> must be a prefix of <code>data</code>. By specifying <code>batch_axis</code> as a <code>PyTree</code> it is possible to specify different batch axes for different leaves of <code>data</code>. (Defaults to <code>0</code>, meaning the first axes of arrays in <code>data</code> are batch dimensions.)</p> <p> TYPE: <code>PyTree[None | int]</code> DEFAULT: <code>0</code> </p> <code>validation_data</code> <p>Arbitrary <code>PyTree</code> used for validation during training. Must have the same tree structure as <code>data</code>. (Defaults to None.)</p> <p> TYPE: <code>PyTree[Any]</code> DEFAULT: <code>None</code> </p> <code>steps</code> <p>Number of gradient updates to apply. (Defaults to 1000.)</p> <p> TYPE: <code>int</code> DEFAULT: <code>1000</code> </p> <code>loss_fn</code> <p>The loss function with call signature <code>(model: PyTree, data: PyTree, batch_axis: int | None | Sequence[Any]) -&gt; float</code>. (Defaults to <code>mse</code>.)</p> <p> TYPE: <code>klax.Loss</code> DEFAULT: <code>&lt;klax._losses.MSE object at 0x7feadbafb800&gt;</code> </p> <code>optimizer</code> <p>The optimizer. Any optax gradient transform to calculate the updates for the model. (Defaults to optax.adam(1e-3).)</p> <p> TYPE: <code>optax.GradientTransformation</code> DEFAULT: <code>GradientTransformationExtraArgs(init=&lt;function chain.&lt;locals&gt;.init_fn&gt;, update=&lt;function chain.&lt;locals&gt;.update_fn&gt;)</code> </p> <code>init_opt_state</code> <p>The initial state of the optimizer. If <code>None</code>, the optimizer is initialized from scratch. By providing a value for <code>init_opt_state</code>, the user can resume training from a previous state (e.g., obtained from the <code>HistoryCallback.last_opt_state</code>). (Defaults to <code>None</code>.)</p> <p> TYPE: <code>PyTree[Any]</code> DEFAULT: <code>None</code> </p> <code>batcher</code> <p>The data loader that splits inputs and targets into batches. (Defaults to <code>batch_data</code>.)</p> <p> TYPE: <code>klax._datahandler.BatchGenerator</code> DEFAULT: <code>&lt;function batch_data&gt;</code> </p> <code>history</code> <p>A callback intended for tracking the training process. If no custom callback is passed the <code>klax.HistoryCallback</code> with a logging interval of 100 steps is used. To change the logging increment or verbosity of this default callback, pass a <code>HistoryCallback</code> object to this argument, e.g., <code>history=HistoryCallback(log_every=10, verbose=False)</code> for logging on every 10-th step without printing the loss.</p> <p> TYPE: <code>klax.HistoryCallback | H | None</code> DEFAULT: <code>None</code> </p> <code>callbacks</code> <p>Callback functions that are evaluated after every training step. They can be used to implement early stopping, custom history logging and more. The argument to the callback function is a CallbackArgs object. (Defaults to <code>None</code>. Keyword only Argument)</p> <p> TYPE: <code>Iterable[klax.Callback] | None</code> DEFAULT: <code>None</code> </p> <code>key</code> <p>A <code>jax.random.PRNGKey</code> used to provide randomness for batch generation. (Keyword only argument.)</p> <p> TYPE: <code>PRNGKeyArray</code> </p> Note <p>This function assumes that the batch dimension is always oriented along the first axes of any <code>jax.Array</code></p> RETURNS DESCRIPTION <code>tuple[T, klax.HistoryCallback | H]</code> <p>A tuple of the trained model and the loss history.</p>"},{"location":"api/wrappers/","title":"Unwrappables and Constraints","text":""},{"location":"api/wrappers/#basic-classes-and-functions","title":"Basic classes and functions","text":""},{"location":"api/wrappers/#klax.Unwrappable","title":"<code>klax.Unwrappable</code> <code></code>","text":"<p>An abstract class representing an unwrappable object.</p> <p>Unwrappables replace PyTree nodes to apply custom behavior upon unwrapping. This class is a renamed copy of <code>paramax.AbstractUnwrappable</code>.</p> Note <p>Models containing Unwrappables need to be unwrapped or finalized before they are callable.</p>"},{"location":"api/wrappers/#klax.Unwrappable.__init__","title":"<code>__init__()</code>","text":"<p>Initialize self.  See help(type(self)) for accurate signature.</p>"},{"location":"api/wrappers/#klax.unwrap","title":"<code>klax.unwrap(tree)</code> <code></code>","text":"<p>Map across a PyTree and unwrap all <code>klax.Unwrappable</code> objects.</p> <p>This leaves all other nodes unchanged. If nested, the innermost <code>klax.Unwrappable</code> is unwrapped first.</p> Example <p>Enforcing positivity.</p> <pre><code>&gt;&gt;&gt; import klax\n&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; params = klax.Parameterize(jnp.exp, jnp.zeros(3))\n&gt;&gt;&gt; klax.unwrap((\"abc\", 1, params))\n('abc', 1, Array([1., 1., 1.], dtype=float32))\n</code></pre>"},{"location":"api/wrappers/#klax.contains_unwrappables","title":"<code>klax.contains_unwrappables(tree)</code> <code></code>","text":"<p>Check if a PyTree contains instances of <code>klax.Unwrappable</code>.</p>"},{"location":"api/wrappers/#klax.Constraint","title":"<code>klax.Constraint(klax.Unwrappable)</code> <code></code>","text":"<p>An abstract constraint around a <code>jax.Array</code>.</p> <p>A <code>klax.Constraint</code> is an extended version of <code>klax.Unwrappable</code>, that marks an array in a PyTree as constrained. It implements the known <code>unwrap</code> method from <code>klax.Unwrappable</code> and adds the <code>apply</code> method for the implementation of constraints that are non-differentiable or could lead to vanishing gradient during optimization.</p> <p>We intend the following usage of the <code>unwrap</code> and <code>apply</code> methods:</p> <p><code>unwrap</code>: Identical functionality to an <code>klax.Unwrappable</code>.     Use this for the implementation of constraints that are differentiable     and shall be applied withing the training loop. E.g., our     implementation of <code>klax.fit</code> will unwrap the model as part of the     loss function. Thus, the implementation of <code>unwrap</code> contributes to the     gradients during training. An example would be a positivity constraint,     that passes the array through <code>jax.nn.softplus</code> upon unwrapping.</p> <p><code>apply</code>: New functionality added with <code>klax.Constraint</code>.     Use this to implement non-differentiable or zero-gradient constraints     that shall be applied <code>after</code> the parameter update and modify the     wrapped array without unwrapping. Consequently, its suitable for the     implementation of <code>non-differentiable</code> constraints, such as clamping     a parameter to a range of admissible values. Apply functions should     return a modified copy of <code>Self</code>.</p> Note <p>Models containing Constraints need to be finalized before they are callable.</p> Warning <p>Constraints objects should not be nested, as this can lead to unexpected behavior or errors. To combine the effects of two constraints, implement a custom constraint and define the combined effects via <code>unwrap</code> and <code>apply</code>.</p>"},{"location":"api/wrappers/#klax.Constraint.__init__","title":"<code>__init__()</code>","text":"<p>Initialize self.  See help(type(self)) for accurate signature.</p>"},{"location":"api/wrappers/#klax.apply","title":"<code>klax.apply(tree)</code> <code></code>","text":"<p>Map across a PyTree and apply all Constraints.</p> <p>This leaves all other nodes unchanged.</p> Example <p>Enforcing non-negativity.</p> <pre><code>&gt;&gt;&gt; import klax\n&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; params = klax.NonNegative(-1 * jnp.ones(3))\n&gt;&gt;&gt; klax.apply((\"abc\", 1, params))\n('abc', 1, NonNegative(parameter=Array([0., 0., 0.], dtype=float32)))\n</code></pre>"},{"location":"api/wrappers/#klax.contains_constraints","title":"<code>klax.contains_constraints(tree)</code> <code></code>","text":"<p>Check if a PyTree contains instances of <code>klax.Constraint</code>.</p>"},{"location":"api/wrappers/#klax.finalize","title":"<code>klax.finalize(tree)</code> <code></code>","text":"<p>Make a model containing Constraints callable.</p> <p>This function combined that functionalities of <code>klax.apply</code> and <code>klax.unwrap</code></p> Warning <p>For models/PyTrees containing Constraints, only <code>finalize</code> the model after the parameter update or after training with <code>klax.fit</code>. This is because <code>klax.finalize</code> returns an unwrapped PyTree where all constraints and wrappers have been applied. However, this also means that the returned PyTree is no longer constrained.</p> <p>If you want to call a model that you want to fit afterwards, we recommend using a different name for the finalized model. For example::</p> <pre><code>&gt;&gt;&gt; finalized_model = klax.finalize(model)\n&gt;&gt;&gt; y = finalzed_model(x)            # Call finalized model\n&gt;&gt;&gt; model, history = fit(model, ...) # Continue training with constrained model\n</code></pre>"},{"location":"api/wrappers/#unwrappables","title":"Unwrappables","text":""},{"location":"api/wrappers/#klax.Parameterize","title":"<code>klax.Parameterize(klax.Unwrappable)</code> <code></code>","text":"<p>Unwrap an object by calling <code>fn</code> with <code>args</code> and <code>`kwargs</code>.</p> <p>All of <code>fn</code>, <code>*args</code> and <code>**kwargs</code> may contain trainable parameters.</p> Note <p>Unwrapping typically occurs after model initialization. Therefore, if the <code>klax.Parameterize</code> object may be created in a vectorized context, we recommend ensuring that <code>fn</code> still unwraps correctly, e.g. by supporting broadcasting.</p> Example <pre><code>&gt;&gt;&gt; from klax import Parameterize, unwrap\n&gt;&gt;&gt; import jax.numpy as jnp\n&gt;&gt;&gt; positive = Parameterize(jnp.exp, jnp.zeros(3))\n&gt;&gt;&gt; unwrap(positive)  # Applies exp on unwrapping\nArray([1., 1., 1.], dtype=float32)\n</code></pre> PARAMETER DESCRIPTION <code>fn</code> <p>Callable to call with args, and kwargs.</p> <p> TYPE: <code>Callable[..., ~T]</code> </p> <code>*args</code> <p>Positional arguments to pass to fn.</p> <p> TYPE: <code>Any</code> </p> <code>**kwargs</code> <p>Keyword arguments to pass to fn.</p> <p> TYPE: <code>Any</code> </p>"},{"location":"api/wrappers/#klax.Parameterize.__init__","title":"<code>__init__(fn, *args, **kwargs)</code> <code></code>","text":""},{"location":"api/wrappers/#klax.NonTrainable","title":"<code>klax.NonTrainable(klax.Unwrappable)</code> <code></code>","text":"<p>Applies stop gradient to all ArrayLike leaves before unwrapping.</p> <p>See also <code>klax.non_trainable</code>, which is probably a generally preferable way to achieve similar behaviour, which wraps the ArrayLike leaves directly, rather than the tree. Useful to mark PyTrees (Arrays, Modules, etc.) as frozen/non-trainable. Note that the underlying parameters may still be impacted by regularization, so it is generally advised to use this as a suggestively named class for filtering parameters.</p>"},{"location":"api/wrappers/#klax.NonTrainable.__init__","title":"<code>__init__(tree)</code>","text":"<p>Initialize self.  See help(type(self)) for accurate signature.</p>"},{"location":"api/wrappers/#klax.non_trainable","title":"<code>klax.non_trainable(tree)</code> <code></code>","text":"<p>Freeze parameters by wrapping inexact arrays.</p> <p>This function wraps a <code>klax.NonTrainable</code> wrapper around every  inexact array or <code>klax.Constraint</code> in the PyTree.</p> Note <p>Regularization is likely to apply before unwrapping. To avoid regularization impacting non-trainable parameters, they should be filtered out, for example using:</p> <pre><code>&gt;&gt;&gt; eqx.partition(\n...     ...,\n...     is_leaf=lambda leaf: isinstance(leaf, (NonTrainable, Constraint)),\n... )\n</code></pre> <p>Wrapping the arrays in a model rather than the entire tree is often preferable, allowing easier access to attributes compared to wrapping the entire tree.</p> PARAMETER DESCRIPTION <code>tree</code> <p>The PyTree.</p> <p> TYPE: <code>PyTree</code> </p>"},{"location":"api/wrappers/#klax.Symmetric","title":"<code>klax.Symmetric(klax.Unwrappable)</code> <code></code>","text":"<p>Ensures symmetry of a square matrix upon unwrapping.</p> Warning <p>Wrapping <code>Symmetric</code> around parameters that are already wrapped may lead to unexpected behavior and is generally discouraged.</p>"},{"location":"api/wrappers/#klax.Symmetric.__init__","title":"<code>__init__(parameter)</code> <code></code>","text":"<p>Initialize a <code>Symmetric</code> wrapper.</p> PARAMETER DESCRIPTION <code>parameter</code> <p>To be wrapped matrix array of shape (..., N, N).</p> <p> TYPE: <code>Array</code> </p>"},{"location":"api/wrappers/#klax.SkewSymmetric","title":"<code>klax.SkewSymmetric(klax.Unwrappable)</code> <code></code>","text":"<p>Ensures skew-symmetry of a square matrix upon unwrapping.</p> Warning <p>Wrapping <code>SkewSymmetric</code> around parameters that are already wrapped may lead to unexpected behavior and is generally discouraged.</p>"},{"location":"api/wrappers/#klax.SkewSymmetric.__init__","title":"<code>__init__(parameter)</code> <code></code>","text":"<p>Initialize a <code>SkewSymmetric</code> wrapper.</p> PARAMETER DESCRIPTION <code>parameter</code> <p>Wrapped matrix as array of shape (..., N, N).</p> <p> TYPE: <code>Array</code> </p>"},{"location":"api/wrappers/#constraints","title":"Constraints","text":""},{"location":"api/wrappers/#klax.NonNegative","title":"<code>klax.NonNegative(klax.Constraint)</code> <code></code>","text":"<p>Applies a non-negative constraint.</p> PARAMETER DESCRIPTION <code>parameter</code> <p>The <code>jax.Array</code> that is to be made non-negative upon unwrapping and applying.</p> <p> TYPE: <code>Array</code> </p>"},{"location":"api/wrappers/#klax.NonNegative.__init__","title":"<code>__init__(parameter)</code>","text":"<p>Initialize self.  See help(type(self)) for accurate signature.</p>"},{"location":"api/nn/linear/","title":"Linear Layers","text":""},{"location":"api/nn/linear/#klax.nn.Linear","title":"<code>klax.nn.Linear</code> <code></code>","text":"<p>Performs a linear transformation.</p> <p>This class is modified from <code>equinox.nn.Linear</code> to allow for custom initialization.</p>"},{"location":"api/nn/linear/#klax.nn.Linear.__init__","title":"<code>__init__(in_features, out_features, weight_init, bias_init=&lt;function zeros&gt;, use_bias=True, weight_wrap=None, bias_wrap=None, dtype=None, *, key)</code> <code></code>","text":"<p>Initialize the linear layer.</p> PARAMETER DESCRIPTION <code>in_features</code> <p>The input size. The input to the layer should be a vector of shape <code>(in_features,)</code></p> <p> TYPE: <code>int | Literal['scalar']</code> </p> <code>out_features</code> <p>The output size. The output from the layer will be a vector of shape <code>(out_features,)</code>.</p> <p> TYPE: <code>int | Literal['scalar']</code> </p> <code>weight_init</code> <p>The weight initializer of type <code>jax.nn.initializers.Initializer</code>.</p> <p> TYPE: <code>jax.nn.initializers.Initializer</code> </p> <code>bias_init</code> <p>The bias initializer of type <code>jax.nn.initializers.Initializer</code>.</p> <p> TYPE: <code>jax.nn.initializers.Initializer</code> DEFAULT: <code>&lt;function zeros&gt;</code> </p> <code>use_bias</code> <p>Whether to add on a bias as well.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>weight_wrap</code> <p>An optional wrapper that can be passed to enforce weight constraints.</p> <p> TYPE: <code>type[klax.Constraint] | type[klax.Unwrappable[Array]] | None</code> DEFAULT: <code>None</code> </p> <code>bias_wrap</code> <p>An optional wrapper that can be passed to enforce bias constraints.</p> <p> TYPE: <code>type[klax.Constraint] | type[klax.Unwrappable[Array]] | None</code> DEFAULT: <code>None</code> </p> <code>dtype</code> <p>The dtype to use for the weight and the bias in this layer. Defaults to either <code>jax.numpy.float32</code> or <code>jax.numpy.float64</code> depending on whether JAX is in 64-bit mode.</p> <p> TYPE: <code>type | None</code> DEFAULT: <code>None</code> </p> <code>key</code> <p>A <code>jax.random.PRNGKey</code> used to provide randomness for parameter initialisation. (Keyword only argument.)</p> <p> TYPE: <code>PRNGKeyArray</code> </p> Note <p>Note that <code>in_features</code> also supports the string <code>\"scalar\"</code> as a special value. In this case the input to the layer should be of shape <code>()</code>.</p> <p>Likewise <code>out_features</code> can also be a string <code>\"scalar\"</code>, in which case the output from the layer will have shape <code>()</code>.</p> <p>Further note that, some <code>jax.nn.initializers.Initializer</code>s do not work if one of <code>in_features</code> or <code>out_features</code> is zero.</p> <p>Likewise, some <code>jax.nn.initializers.Initialzers</code>s do not work when <code>dtype</code> is <code>jax.numpy.complex64</code>.</p>"},{"location":"api/nn/linear/#klax.nn.Linear.__call__","title":"<code>__call__(x, *, key=None)</code> <code></code>","text":"<p>Forward pass of the linear transformation.</p> PARAMETER DESCRIPTION <code>x</code> <p>The input. Should be a JAX array of shape <code>(in_features,)</code>. (Or shape <code>()</code> if <code>in_features=\"scalar\"</code>.)</p> <p> TYPE: <code>Array</code> </p> <code>key</code> <p>Ignored; provided for compatibility with the rest of the Equinox API. (Keyword only argument.)</p> <p> TYPE: <code>PRNGKeyArray | None</code> DEFAULT: <code>None</code> </p> Note <p>If you want to use higher order tensors as inputs (for example featuring batch dimensions) then use <code>jax.vmap</code>. For example, for an input <code>x</code> of shape <code>(batch, in_features)</code>, using</p> <pre><code>&gt;&gt;&gt; import jax\n&gt;&gt;&gt; from jax.nn.initializers import he_normal\n&gt;&gt;&gt; import jax.random as jrandom\n&gt;&gt;&gt; import klax\n&gt;&gt;&gt;\n&gt;&gt;&gt; key = jrandom.PRNGKey(0)\n&gt;&gt;&gt; keys = jrandom.split(key)\n&gt;&gt;&gt; x = jrandom.uniform(keys[0], (10,))\n&gt;&gt;&gt; linear = klax.nn.Linear(\n&gt;&gt;&gt;     \"scalar\",\n&gt;&gt;&gt;     \"scalar\",\n&gt;&gt;&gt;     he_normal(),\n&gt;&gt;&gt;     key=keys[1]\n&gt;&gt;&gt; )\n&gt;&gt;&gt; jax.vmap(linear)(x).shape\n(10,)\n</code></pre> <p>will produce the appropriate output of shape <code>(batch, out_features)</code>.</p> RETURNS DESCRIPTION <code>Array</code> <p>A JAX array of shape <code>(out_features,)</code>. (Or shape <code>()</code> if</p> <code>Array</code> <p><code>out_features=\"scalar\"</code>.)</p>"},{"location":"api/nn/linear/#klax.nn.InputSplitLinear","title":"<code>klax.nn.InputSplitLinear</code> <code></code>","text":"<p>Performs a linear transformation for multiple inputs.</p> <p>The transformation is of the form: <code>y = x_1 @ W_1 + x_2 @ W_2 + ... + x_n @ W_n + b</code> for <code>x_1, ..., x_n</code>.</p> <p>This layer is useful for formulating transformations with multiple inputs where different inputs require different weight constraints or initialization for the corresponding weight matrices.</p>"},{"location":"api/nn/linear/#klax.nn.InputSplitLinear.__init__","title":"<code>__init__(in_features, out_features, weight_inits, bias_init=&lt;function zeros&gt;, use_bias=True, weight_wraps=None, bias_wrap=None, dtype=None, *, key)</code> <code></code>","text":"<p>Initialize the input split linear layer.</p> PARAMETER DESCRIPTION <code>in_features</code> <p>The input sizes of each input. The n-th input to the layer should be a vector of shape <code>(in_features[n],)</code></p> <p> TYPE: <code>Sequence[int | Literal['scalar']]</code> </p> <code>out_features</code> <p>The output size. The output from the layer will be a vector of shape <code>(out_features,)</code>.</p> <p> TYPE: <code>int | Literal['scalar']</code> </p> <code>weight_inits</code> <p>Weight initializer or sequence of weight initializers of type <code>jax.nn.initializers.Initializer</code>. By specifying a sequence it is possible to apply a different initializer to each weight matrix. The sequence must have the same length as in_features.</p> <p> TYPE: <code>Sequence[jax.nn.initializers.Initializer] | jax.nn.initializers.Initializer</code> </p> <code>bias_init</code> <p>The bias initializer of type <code>jax.nn.initializers.Initializer</code>.</p> <p> TYPE: <code>jax.nn.initializers.Initializer</code> DEFAULT: <code>&lt;function zeros&gt;</code> </p> <code>use_bias</code> <p>Whether to add on a bias as well.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>weight_wraps</code> <p>One or a list/tuple of wrappers that can be passed to enforce weight constraints. By specifying a sequence it is possible to apply a different wrapper to each weight matrix. The sequence must have the same length as in_features.</p> <p> TYPE: <code>Sequence[type[klax.Constraint] | type[klax.Unwrappable[Array]] | None] | type[klax.Constraint] | type[klax.Unwrappable[Array]] | None</code> DEFAULT: <code>None</code> </p> <code>bias_wrap</code> <p>An optional wrapper that can be passed to enforce bias constraints.</p> <p> TYPE: <code>type[klax.Constraint] | type[klax.Unwrappable[Array]] | None</code> DEFAULT: <code>None</code> </p> <code>dtype</code> <p>The dtype to use for the weight and the bias in this layer. Defaults to either <code>jax.numpy.float32</code> or <code>jax.numpy.float64</code> depending on whether JAX is in 64-bit mode.</p> <p> TYPE: <code>type | None</code> DEFAULT: <code>None</code> </p> <code>key</code> <p>A <code>jax.random.PRNGKey</code> used to provide randomness for parameter initialisation. (Keyword only argument.)</p> <p> TYPE: <code>PRNGKeyArray</code> </p> Note <p>Note that <code>in_features</code> also supports the string <code>\"scalar\"</code> as a special value. In this case the respective input to the layer should be of shape <code>()</code>.</p> <p>Likewise <code>out_features</code> can also be a string <code>\"scalar\"</code>, in which case the output from the layer will have shape <code>()</code>.</p> <p>Further note that, some <code>jax.nn.initializers.Initializer</code>s do not work if one of <code>in_features</code> or <code>out_features</code> is zero.</p> <p>Likewise, some <code>jax.nn.initializers.Initialzer</code>s do not work when <code>dtype</code> is <code>jax.numpy.complex64</code>.</p>"},{"location":"api/nn/linear/#klax.nn.InputSplitLinear.__call__","title":"<code>__call__(*xs, key=None)</code> <code></code>","text":"<p>Forward pass of the linear transformation.</p> PARAMETER DESCRIPTION <code>xs</code> <p>The inputs. Should be n JAX arrays x_i of shape <code>(in_features[i],)</code>. (Or shape <code>()</code> if <code>in_features[i]=\"scalar\"</code>.)</p> <p> TYPE: <code>Array</code> </p> <code>key</code> <p>Ignored; provided for compatibility with the rest of the Equinox API. (Keyword only argument.)</p> <p> TYPE: <code>PRNGKeyArray | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Array</code> <p>A JAX array of shape <code>(out_features,)</code>. (Or shape <code>()</code> if</p> <code>Array</code> <p><code>out_features=\"scalar\"</code>.)</p>"},{"location":"api/nn/matrices/","title":"Matrix-valued functions","text":""},{"location":"api/nn/matrices/#klax.nn.Matrix","title":"<code>klax.nn.Matrix</code> <code></code>","text":"<p>An unconstrained matrix-valued function based on an MLP.</p> <p>The MLP maps to a vector of elements which is transformed into a matrix.</p>"},{"location":"api/nn/matrices/#klax.nn.Matrix.__init__","title":"<code>__init__(in_size, shape, width_sizes, weight_init=&lt;function variance_scaling.&lt;locals&gt;.init&gt;, bias_init=&lt;function zeros&gt;, activation=&lt;PjitFunction of &lt;function softplus at 0x7feadbb39440&gt;&gt;, final_activation=&lt;function Matrix.&lt;lambda&gt;&gt;, use_bias=True, use_final_bias=True, dtype=None, *, key)</code> <code></code>","text":"<p>Initialize the <code>Matrix</code>.</p> PARAMETER DESCRIPTION <code>in_size</code> <p>The input size. The input to the module should be a vector of shape <code>(in_size,)</code></p> <p> TYPE: <code>int | Literal['scalar']</code> </p> <code>shape</code> <p>The matrix shape. The output from the module will be an array with the specified <code>shape</code>. For square matrices a single integer N can be used as a shorthand for (N, N).</p> <p> TYPE: <code>int | AtLeast2DTuple[int]</code> </p> <code>width_sizes</code> <p>The sizes of each hidden layer of the underlying MLP in a list.</p> <p> TYPE: <code>Sequence[int]</code> </p> <code>weight_init</code> <p>The weight initializer of type <code>jax.nn.initializers.Initializer</code>. (Defaults to <code>he_normal()</code>)</p> <p> TYPE: <code>jax.nn.initializers.Initializer</code> DEFAULT: <code>&lt;function variance_scaling.&lt;locals&gt;.init&gt;</code> </p> <code>bias_init</code> <p>The bias initializer of type <code>jax.nn.initializers.Initializer</code>. (Defaults to <code>zeros</code>)</p> <p> TYPE: <code>jax.nn.initializers.Initializer</code> DEFAULT: <code>&lt;function zeros&gt;</code> </p> <code>activation</code> <p>The activation function after each hidden layer. (Defaults to ReLU).</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>&lt;PjitFunction of &lt;function softplus at 0x7feadbb39440&gt;&gt;</code> </p> <code>final_activation</code> <p>The activation function after the output layer. (Defaults to the identity.)</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>&lt;function Matrix.&lt;lambda&gt;&gt;</code> </p> <code>use_bias</code> <p>Whether to add on a bias to internal layers. (Defaults to <code>True</code>.)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>use_final_bias</code> <p>Whether to add on a bias to the final layer. (Defaults to <code>True</code>.)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dtype</code> <p>The dtype to use for all the weights and biases in this MLP. (Defaults to either <code>jax.numpy.float32</code> or <code>jax.numpy.float64</code> depending on whether JAX is in 64-bit mode.)</p> <p> TYPE: <code>Any | None</code> DEFAULT: <code>None</code> </p> <code>key</code> <p>A <code>jax.random.PRNGKey</code> used to provide randomness for parameter initialisation. (Keyword only argument.)</p> <p> TYPE: <code>PRNGKeyArray</code> </p> Note <p>Note that <code>in_size</code> also supports the string <code>\"scalar\"</code> as a special value. In this case the input to the module should be of shape <code>()</code>.</p>"},{"location":"api/nn/matrices/#klax.nn.Matrix.__call__","title":"<code>__call__(x)</code> <code></code>","text":"<p>Forward pass through <code>Matrix</code>.</p> PARAMETER DESCRIPTION <code>x</code> <p>The input. Should be a JAX array of shape <code>(in_size,)</code>. (Or shape <code>()</code> if <code>in_size=\"scalar\"</code>.)</p> <p> TYPE: <code>Array</code> </p> RETURNS DESCRIPTION <code>Array</code> <p>A JAX array of shape <code>shape</code>.</p>"},{"location":"api/nn/matrices/#klax.nn.ConstantMatrix","title":"<code>klax.nn.ConstantMatrix</code> <code></code>","text":"<p>A constant, unconstrained matrix.</p> <p>It is a wrapper around a constant array that implements the matrix-valued function interface.</p>"},{"location":"api/nn/matrices/#klax.nn.ConstantMatrix.__init__","title":"<code>__init__(shape, init=&lt;function variance_scaling.&lt;locals&gt;.init&gt;, dtype=None, *, key)</code> <code></code>","text":"<p>Initialize the <code>ConstantMatrix</code>.</p> PARAMETER DESCRIPTION <code>shape</code> <p>The matrix shape. The output from the module will be a Array with sthe specified <code>shape</code>. For square matrices a single integer N can be used as a shorthand for (N, N).</p> <p> TYPE: <code>int | AtLeast2DTuple[int]</code> </p> <code>init</code> <p>The array initializer of type <code>jax.nn.initializers.Initializer</code>. (Defaults to <code>variance_scaling(scale=1, mode=\"fan_avg\", distribution=\"normal\")</code>.)</p> <p> TYPE: <code>jax.nn.initializers.Initializer</code> DEFAULT: <code>&lt;function variance_scaling.&lt;locals&gt;.init&gt;</code> </p> <code>dtype</code> <p>The dtype to use for all the weights and biases in this MLP. (Defaults to either <code>jax.numpy.float32</code> or <code>jax.numpy.float64</code> depending on whether JAX is in 64-bit mode.)</p> <p> TYPE: <code>Any | None</code> DEFAULT: <code>None</code> </p> <code>key</code> <p>A <code>jax.random.PRNGKey</code> used to provide randomness for parameter initialisation. (Keyword only argument.)</p> <p> TYPE: <code>PRNGKeyArray</code> </p>"},{"location":"api/nn/matrices/#klax.nn.ConstantMatrix.__call__","title":"<code>__call__(x)</code> <code></code>","text":"<p>Forward pass through <code>ConstantMatrix</code>.</p> PARAMETER DESCRIPTION <code>x</code> <p>Ignored; provided for compatibility with the rest of the Matrix-valued function API.</p> <p> TYPE: <code>Array</code> </p> RETURNS DESCRIPTION <code>Array</code> <p>A JAX array of shape <code>shape</code>.</p>"},{"location":"api/nn/matrices/#klax.nn.SkewSymmetricMatrix","title":"<code>klax.nn.SkewSymmetricMatrix</code> <code></code>","text":"<p>A kkew-symmetric matrix-valued function based on an MLP.</p> <p>The MLP maps the input to a vector of elements that are transformed into a skew-symmetric matrix.</p>"},{"location":"api/nn/matrices/#klax.nn.SkewSymmetricMatrix.__init__","title":"<code>__init__(in_size, shape, width_sizes, weight_init=&lt;function variance_scaling.&lt;locals&gt;.init&gt;, bias_init=&lt;function zeros&gt;, activation=&lt;PjitFunction of &lt;function softplus at 0x7feadbb39440&gt;&gt;, final_activation=&lt;function SkewSymmetricMatrix.&lt;lambda&gt;&gt;, use_bias=True, use_final_bias=True, dtype=None, *, key)</code> <code></code>","text":"<p>Initialize the <code>SkewSymmetricMatrix</code>.</p> PARAMETER DESCRIPTION <code>in_size</code> <p>The input size. The input to the module should be a vector of shape <code>(in_size,)</code></p> <p> TYPE: <code>int | Literal['scalar']</code> </p> <code>shape</code> <p>The matrix shape. The output from the module will be a Array with sthe specified <code>shape</code>. For square matrices a single integer N can be used as a shorthand for (N, N).</p> <p> TYPE: <code>int | AtLeast2DTuple[int]</code> </p> <code>width_sizes</code> <p>The sizes of each hidden layer of the underlying MLP in a list.</p> <p> TYPE: <code>Sequence[int]</code> </p> <code>weight_init</code> <p>The weight initializer of type <code>jax.nn.initializers.Initializer</code>. (Defaults to <code>he_normal()</code>)</p> <p> TYPE: <code>jax.nn.initializers.Initializer</code> DEFAULT: <code>&lt;function variance_scaling.&lt;locals&gt;.init&gt;</code> </p> <code>bias_init</code> <p>The bias initializer of type <code>jax.nn.initializers.Initializer</code>. (Defaults to <code>zeros</code>)</p> <p> TYPE: <code>jax.nn.initializers.Initializer</code> DEFAULT: <code>&lt;function zeros&gt;</code> </p> <code>activation</code> <p>The activation function after each hidden layer. (Defaults to <code>softplus</code>).</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>&lt;PjitFunction of &lt;function softplus at 0x7feadbb39440&gt;&gt;</code> </p> <code>final_activation</code> <p>The activation function after the output layer. (Defaults to the identity.)</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>&lt;function SkewSymmetricMatrix.&lt;lambda&gt;&gt;</code> </p> <code>use_bias</code> <p>Whether to add on a bias to internal layers. (Defaults to <code>True</code>.)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>use_final_bias</code> <p>Whether to add on a bias to the final layer. (Defaults to <code>True</code>.)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dtype</code> <p>The dtype to use for all the weights and biases in this MLP. (Defaults to either <code>jax.numpy.float32</code> or <code>jax.numpy.float64</code> depending on whether JAX is in 64-bit mode.)</p> <p> TYPE: <code>Any | None</code> DEFAULT: <code>None</code> </p> <code>key</code> <p>A <code>jax.random.PRNGKey</code> used to provide randomness for parameter initialisation. (Keyword only argument.)</p> <p> TYPE: <code>PRNGKeyArray</code> </p> Note <p>Note that <code>in_size</code> also supports the string <code>\"scalar\"</code> as a special value. In this case the input to the module should be of shape <code>()</code>.</p>"},{"location":"api/nn/matrices/#klax.nn.SkewSymmetricMatrix.__call__","title":"<code>__call__(x)</code> <code></code>","text":"<p>Forward pass through <code>SkewSymmetricMatrix</code>.</p> PARAMETER DESCRIPTION <code>x</code> <p>The input. Should be a JAX array of shape <code>(in_size,)</code>. (Or shape <code>()</code> if <code>in_size=\"scalar\"</code>.)</p> <p> TYPE: <code>Array</code> </p> RETURNS DESCRIPTION <code>Array</code> <p>A JAX array of shape <code>shape</code>.</p>"},{"location":"api/nn/matrices/#klax.nn.ConstantSkewSymmetricMatrix","title":"<code>klax.nn.ConstantSkewSymmetricMatrix</code> <code></code>","text":"<p>A constant skew-symmetric matrix.</p> <p>It is a wrapper around a constant skew-symmetry-constraind array that implements the matrix-valued function interface.</p>"},{"location":"api/nn/matrices/#klax.nn.ConstantSkewSymmetricMatrix.__init__","title":"<code>__init__(shape, init=&lt;function variance_scaling.&lt;locals&gt;.init&gt;, dtype=None, *, key)</code> <code></code>","text":"<p>Initialize the <code>ConstantSkewSymmetricMatrix</code>.</p> PARAMETER DESCRIPTION <code>shape</code> <p>The matrix shape. The output from the module will be a Array with sthe specified <code>shape</code>. For square matrices a single integer N can be used as a shorthand for (N, N).</p> <p> TYPE: <code>int | AtLeast2DTuple[int]</code> </p> <code>init</code> <p>The array initializer of type <code>jax.nn.initializers.Initializer</code>. (Defaults to <code>variance_scaling(scale=1, mode=\"fan_avg\", distribution=\"normal\")</code>.)</p> <p> TYPE: <code>jax.nn.initializers.Initializer</code> DEFAULT: <code>&lt;function variance_scaling.&lt;locals&gt;.init&gt;</code> </p> <code>dtype</code> <p>The dtype to use for all the weights and biases in this MLP. (Defaults to either <code>jax.numpy.float32</code> or <code>jax.numpy.float64</code> depending on whether JAX is in 64-bit mode.)</p> <p> TYPE: <code>Any | None</code> DEFAULT: <code>None</code> </p> <code>key</code> <p>A <code>jax.random.PRNGKey</code> used to provide randomness for parameter initialisation. (Keyword only argument.)</p> <p> TYPE: <code>PRNGKeyArray</code> </p>"},{"location":"api/nn/matrices/#klax.nn.ConstantSkewSymmetricMatrix.__call__","title":"<code>__call__(x)</code> <code></code>","text":"<p>Forward pass through <code>ConstantSkewSymmetricMatrix</code>.</p> PARAMETER DESCRIPTION <code>x</code> <p>Ignored; provided for compatibility with the rest of the Matrix-valued function API.</p> <p> TYPE: <code>Array</code> </p> RETURNS DESCRIPTION <code>Array</code> <p>A JAX array of shape <code>shape</code>.</p>"},{"location":"api/nn/matrices/#klax.nn.SPDMatrix","title":"<code>klax.nn.SPDMatrix</code> <code></code>","text":"<p>A symmetric positive definite matrix-valued function based on an MLP.</p> <p>The output vector <code>v</code> of the MLP is mapped to a matrix <code>B</code>. The module's output is then computed via <code>A=B@B*</code>.</p>"},{"location":"api/nn/matrices/#klax.nn.SPDMatrix.__init__","title":"<code>__init__(in_size, shape, width_sizes, epsilon=1e-06, weight_init=&lt;function variance_scaling.&lt;locals&gt;.init&gt;, bias_init=&lt;function zeros&gt;, activation=&lt;PjitFunction of &lt;function softplus at 0x7feadbb39440&gt;&gt;, final_activation=&lt;function SPDMatrix.&lt;lambda&gt;&gt;, use_bias=True, use_final_bias=True, dtype=None, *, key)</code> <code></code>","text":"<p>Initialize the <code>SPDMatrix</code>.</p> PARAMETER DESCRIPTION <code>in_size</code> <p>The input size. The input to the module should be a vector of shape <code>(in_size,)</code></p> <p> TYPE: <code>int | Literal['scalar']</code> </p> <code>shape</code> <p>The matrix shape. The output from the module will be a Array with sthe specified <code>shape</code>. For square matrices a single integer N can be used as a shorthand for (N, N).</p> <p> TYPE: <code>int | AtLeast2DTuple[int]</code> </p> <code>width_sizes</code> <p>The sizes of each hidden layer of the underlying MLP in a list.</p> <p> TYPE: <code>Sequence[int]</code> </p> <code>epsilon</code> <p>Small value that is added to the diagonal of the output matrix to ensure positive definiteness. If only positive semi-definiteness is required set <code>epsilon = 0.</code> (Defaults to <code>1e-6</code>)</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-06</code> </p> <code>weight_init</code> <p>The weight initializer of type <code>jax.nn.initializers.Initializer</code>. (Defaults to <code>he_normal()</code>)</p> <p> TYPE: <code>jax.nn.initializers.Initializer</code> DEFAULT: <code>&lt;function variance_scaling.&lt;locals&gt;.init&gt;</code> </p> <code>bias_init</code> <p>The bias initializer of type <code>jax.nn.initializers.Initializer</code>. (Defaults to <code>zeros</code>)</p> <p> TYPE: <code>jax.nn.initializers.Initializer</code> DEFAULT: <code>&lt;function zeros&gt;</code> </p> <code>activation</code> <p>The activation function after each hidden layer. (Defaults to <code>softplus</code>)</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>&lt;PjitFunction of &lt;function softplus at 0x7feadbb39440&gt;&gt;</code> </p> <code>final_activation</code> <p>The activation function after the output layer. (Defaults to the identity.)</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>&lt;function SPDMatrix.&lt;lambda&gt;&gt;</code> </p> <code>use_bias</code> <p>Whether to add on a bias to internal layers. (Defaults to <code>True</code>.)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>use_final_bias</code> <p>Whether to add on a bias to the final layer. (Defaults to <code>True</code>.)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dtype</code> <p>The dtype to use for all the weights and biases in this MLP. (Defaults to either <code>jax.numpy.float32</code> or <code>jax.numpy.float64</code> depending on whether JAX is in 64-bit mode.)</p> <p> TYPE: <code>Any | None</code> DEFAULT: <code>None</code> </p> <code>key</code> <p>A <code>jax.random.PRNGKey</code> used to provide randomness for parameter initialisation. (Keyword only argument.)</p> <p> TYPE: <code>PRNGKeyArray</code> </p> Note <p>Note that <code>in_size</code> also supports the string <code>\"scalar\"</code> as a special value. In this case the input to the module should be of shape <code>()</code>.</p>"},{"location":"api/nn/matrices/#klax.nn.SPDMatrix.__call__","title":"<code>__call__(x)</code> <code></code>","text":"<p>Forward pass through <code>SPDMatrix</code>.</p> PARAMETER DESCRIPTION <code>x</code> <p>The input. Should be a JAX array of shape <code>(in_size,)</code>. (Or shape <code>()</code> if <code>in_size=\"scalar\"</code>.)</p> <p> TYPE: <code>Array</code> </p> RETURNS DESCRIPTION <code>Array</code> <p>A JAX array of shape <code>shape</code>.</p>"},{"location":"api/nn/matrices/#klax.nn.ConstantSPDMatrix","title":"<code>klax.nn.ConstantSPDMatrix</code> <code></code>","text":"<p>A constant symmetric positive definite matrix-valued function.</p> <p>It is a wrapper around a constant symmetric postive semi-definite matrix with the matrix-valued function interface.</p>"},{"location":"api/nn/matrices/#klax.nn.ConstantSPDMatrix.__init__","title":"<code>__init__(shape, epsilon=1e-06, init=&lt;function variance_scaling.&lt;locals&gt;.init&gt;, dtype=None, *, key)</code> <code></code>","text":"<p>Initialize the <code>ConstantSPDMatrix</code>.</p> PARAMETER DESCRIPTION <code>shape</code> <p>The matrix shape. The output from the module will be a Array with sthe specified <code>shape</code>. For square matrices a single integer N can be used as a shorthand for (N, N).</p> <p> TYPE: <code>int | AtLeast2DTuple[int]</code> </p> <code>epsilon</code> <p>Small value that is added to the diagonal of the output matrix to ensure positive definiteness. If only positive semi-definiteness is required set <code>epsilon = 0.</code> (Defaults to <code>1e-6</code>)</p> <p> TYPE: <code>float</code> DEFAULT: <code>1e-06</code> </p> <code>init</code> <p>The initializer of type <code>jax.nn.initializers.Initializer</code> for the constant matrix <code>B</code> that produces the module's output via <code>A = B@B*</code>. (Defaults to <code>variance_scaling(scale=1, mode=\"fan_avg\", distribution=\"normal\")</code>.)</p> <p> TYPE: <code>jax.nn.initializers.Initializer</code> DEFAULT: <code>&lt;function variance_scaling.&lt;locals&gt;.init&gt;</code> </p> <code>dtype</code> <p>The dtype to use for all the weights and biases in this MLP. (Defaults to either <code>jax.numpy.float32</code> or <code>jax.numpy.float64</code> depending on whether JAX is in 64-bit mode.)</p> <p> TYPE: <code>Any | None</code> DEFAULT: <code>None</code> </p> <code>key</code> <p>A <code>jax.random.PRNGKey</code> used to provide randomness for parameter initialisation. (Keyword only argument.)</p> <p> TYPE: <code>PRNGKeyArray</code> </p>"},{"location":"api/nn/matrices/#klax.nn.ConstantSPDMatrix.__call__","title":"<code>__call__(x)</code> <code></code>","text":"<p>Forward pass through <code>ConstantSPDMatrix</code>.</p> PARAMETER DESCRIPTION <code>x</code> <p>Ignored; provided for compatibility with the rest of the matrix-valued function API.</p> <p> TYPE: <code>Array</code> </p> RETURNS DESCRIPTION <code>Array</code> <p>A JAX array of shape <code>shape</code>.</p>"},{"location":"api/nn/mlp/","title":"Multi-layer perceptrons","text":""},{"location":"api/nn/mlp/#klax.nn.MLP","title":"<code>klax.nn.MLP</code> <code></code>","text":"<p>Standard Multi-Layer Perceptron; also known as a feed-forward network.</p> <p>This class is modified form <code>equinox.nn.MLP</code> to allow for custom initialization and different node numbers in the hidden layers. Hence, it may also be used for ecoder/decoder tasks.</p>"},{"location":"api/nn/mlp/#klax.nn.MLP.__init__","title":"<code>__init__(in_size, out_size, width_sizes, weight_init=&lt;function variance_scaling.&lt;locals&gt;.init&gt;, bias_init=&lt;function zeros&gt;, activation=&lt;PjitFunction of &lt;function softplus at 0x7feadbb39440&gt;&gt;, final_activation=&lt;function MLP.&lt;lambda&gt;&gt;, use_bias=True, use_final_bias=True, weight_wrap=None, bias_wrap=None, dtype=None, *, key)</code> <code></code>","text":"<p>Initialize MLP.</p> PARAMETER DESCRIPTION <code>in_size</code> <p>The input size. The input to the module should be a vector of shape <code>(in_features,)</code>.</p> <p> TYPE: <code>int | Literal['scalar']</code> </p> <code>out_size</code> <p>The output size. The output from the module will be a vector of shape <code>(out_features,)</code>.</p> <p> TYPE: <code>int | Literal['scalar']</code> </p> <code>width_sizes</code> <p>The sizes of each hidden layer in a list.</p> <p> TYPE: <code>Sequence[int]</code> </p> <code>weight_init</code> <p>The weight initializer of type <code>jax.nn.initializers.Initializer</code>. (Defaults to he_normal().)</p> <p> TYPE: <code>jax.nn.initializers.Initializer</code> DEFAULT: <code>&lt;function variance_scaling.&lt;locals&gt;.init&gt;</code> </p> <code>bias_init</code> <p>The bias initializer of type <code>jax.nn.initializers.Initializer</code>. (Defaults to zeros.)</p> <p> TYPE: <code>jax.nn.initializers.Initializer</code> DEFAULT: <code>&lt;function zeros&gt;</code> </p> <code>activation</code> <p>The activation function after each hidden layer. (Defaults to <code>jax.nn.softplus</code>).</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>&lt;PjitFunction of &lt;function softplus at 0x7feadbb39440&gt;&gt;</code> </p> <code>final_activation</code> <p>The activation function after the output layer. (Defaults to the identity.)</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>&lt;function MLP.&lt;lambda&gt;&gt;</code> </p> <code>use_bias</code> <p>Whether to add on a bias to internal layers. (Defaults to <code>True</code>.)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>use_final_bias</code> <p>Whether to add on a bias to the final layer. (Defaults to <code>True</code>.)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>weight_wrap</code> <p>An optional wrapper that is passed to all weights.</p> <p> TYPE: <code>type[klax.Constraint] | type[klax.Unwrappable[Array]] | None</code> DEFAULT: <code>None</code> </p> <code>bias_wrap</code> <p>An optional wrapper that is passed to all biases.</p> <p> TYPE: <code>type[klax.Constraint] | type[klax.Unwrappable[Array]] | None</code> DEFAULT: <code>None</code> </p> <code>dtype</code> <p>The dtype to use for all the weights and biases in this MLP. Defaults to either <code>jax.numpy.float32</code> or <code>jax.numpy.float64</code> depending on whether JAX is in 64-bit mode.</p> <p> TYPE: <code>type | None</code> DEFAULT: <code>None</code> </p> <code>key</code> <p>A <code>jax.random.PRNGKey</code> used to provide randomness for parameter initialisation. (Keyword only argument.)</p> <p> TYPE: <code>PRNGKeyArray</code> </p> Note <p>Note that <code>in_size</code> also supports the string <code>\"scalar\"</code> as a special value. In this case the input to the module should be of shape <code>()</code>.</p> <p>Likewise <code>out_size</code> can also be a string <code>\"scalar\"</code>, in which case the output from the module will have shape <code>()</code>.</p>"},{"location":"api/nn/mlp/#klax.nn.MLP.__call__","title":"<code>__call__(x, *, key=None)</code> <code></code>","text":"<p>Forward pass through MLP.</p> PARAMETER DESCRIPTION <code>x</code> <p>A JAX array with shape <code>(in_size,)</code>. (Or shape <code>()</code> if <code>in_size=\"scalar\"</code>.)</p> <p> TYPE: <code>Array</code> </p> <code>key</code> <p>Ignored; provided for compatibility with the rest of the Equinox API. (Keyword only argument.)</p> <p> TYPE: <code>PRNGKeyArray | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Array</code> <p>A JAX array with shape <code>(out_size,)</code>. (Or shape <code>()</code> if</p> <code>Array</code> <p><code>out_size=\"scalar\"</code>.)</p>"},{"location":"api/nn/mlp/#klax.nn.FICNN","title":"<code>klax.nn.FICNN</code> <code></code>","text":"<p>A fully input convex neural network (FICNN).</p> <p>Each element of the output is a convex function of the input.</p> <p>See: https://arxiv.org/abs/1609.07152</p>"},{"location":"api/nn/mlp/#klax.nn.FICNN.__init__","title":"<code>__init__(in_size, out_size, width_sizes, use_passthrough=True, non_decreasing=False, weight_init=&lt;function variance_scaling.&lt;locals&gt;.init&gt;, bias_init=&lt;function zeros&gt;, activation=&lt;PjitFunction of &lt;function softplus at 0x7feadbb39440&gt;&gt;, final_activation=&lt;function FICNN.&lt;lambda&gt;&gt;, use_bias=True, use_final_bias=True, dtype=None, *, key)</code> <code></code>","text":"<p>Initialize FICNN.</p> Warning <p>Modifying <code>final_activation</code> to a non-convex function will break the convexity of the FICNN. Use this parameter with care.</p> PARAMETER DESCRIPTION <code>in_size</code> <p>The input size. The input to the module should be a vector of shape <code>(in_features,)</code>.</p> <p> TYPE: <code>int | Literal['scalar']</code> </p> <code>out_size</code> <p>The output size. The output from the module will be a vector of shape <code>(out_features,)</code>.</p> <p> TYPE: <code>int | Literal['scalar']</code> </p> <code>width_sizes</code> <p>The sizes of each hidden layer in a list.</p> <p> TYPE: <code>Sequence[int]</code> </p> <code>use_passthrough</code> <p>Whether to use passthrough layers. If true, the input is passed through to each hidden layer. Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>non_decreasing</code> <p>If true, the output is element-wise non-decreasing in each input. This is useful if the input <code>x</code> is a convex function of some other quantity <code>z</code>. If the FICNN <code>f(x(z))</code> is non-decreasing then f preserves the convexity with respect to <code>z</code>. Defaults to False.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>False</code> </p> <code>weight_init</code> <p>The weight initializer of type <code>jax.nn.initializers.Initializer</code>. Defaults to he_normal().</p> <p> TYPE: <code>jax.nn.initializers.Initializer</code> DEFAULT: <code>&lt;function variance_scaling.&lt;locals&gt;.init&gt;</code> </p> <code>bias_init</code> <p>The bias initializer of type <code>jax.nn.initializers.Initializer</code>. Defaults to zeros.</p> <p> TYPE: <code>jax.nn.initializers.Initializer</code> DEFAULT: <code>&lt;function zeros&gt;</code> </p> <code>activation</code> <p>The activation function of each hidden layer. To ensure convexity this function must be convex and non-decreasing. Defaults to <code>jax.nn.softplus</code>.</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>&lt;PjitFunction of &lt;function softplus at 0x7feadbb39440&gt;&gt;</code> </p> <code>final_activation</code> <p>The activation function after the output layer. To ensure convexity this function must be convex and non-decreasing. (Defaults to the identity.)</p> <p> TYPE: <code>Callable</code> DEFAULT: <code>&lt;function FICNN.&lt;lambda&gt;&gt;</code> </p> <code>use_bias</code> <p>Whether to add on a bias in the hidden layers. (Defaults to True.)</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>use_final_bias</code> <p>Whether to add on a bias to the final layer. Defaults to True.</p> <p> TYPE: <code>bool</code> DEFAULT: <code>True</code> </p> <code>dtype</code> <p>The dtype to use for all the weights and biases in this MLP. Defaults to either <code>jax.numpy.float32</code> or <code>jax.numpy.float64</code> depending on whether JAX is in 64-bit mode.</p> <p> TYPE: <code>type | None</code> DEFAULT: <code>None</code> </p> <code>key</code> <p>A <code>jax.random.PRNGKey</code> used to provide randomness for parameter initialisation. (Keyword only argument.)</p> <p> TYPE: <code>PRNGKeyArray</code> </p>"},{"location":"api/nn/mlp/#klax.nn.FICNN.__call__","title":"<code>__call__(x, *, key=None)</code> <code></code>","text":"<p>Forward pass through <code>FICNN</code>.</p> PARAMETER DESCRIPTION <code>x</code> <p>A JAX array with shape <code>(in_size,)</code>. (Or shape <code>()</code> if <code>in_size=\"scalar\"</code>.)</p> <p> TYPE: <code>Array</code> </p> <code>key</code> <p>Ignored; provided for compatibility with the rest of the Equinox API. (Keyword only argument.)</p> <p> TYPE: <code>PRNGKeyArray | None</code> DEFAULT: <code>None</code> </p> RETURNS DESCRIPTION <code>Array</code> <p>A JAX array with shape <code>(out_size,)</code>. (Or shape <code>()</code> if</p> <code>Array</code> <p><code>out_size=\"scalar\"</code>.)</p>"},{"location":"examples/isotropic_hyperelasticity/","title":"Isotropic Hyperelasticity","text":"<p>This notebook implements and calibrates an neural network-based isotropic, incompressible hyperelastic material model to Treloar's experimental data using the method presented in Damma\u00df et al. (2025). It showcases the basic use of <code>klax</code>'s input covex neural network (<code>FICNN</code>) and <code>klax.fit()</code> function for building and calibrating physics-augmented neural network models.</p> <p>To run it locally install klax with plotting capability via <code>pip install 'klax[plot]'</code>.</p> <p>We start by importing the required packages for model creation, optimization and plotting. We also import some type aliases and import the <code>dataclass</code> module for some simple data handling.</p> <pre><code>from dataclasses import dataclass\n\nimport equinox as eqx\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jrandom\nimport optax\nfrom jaxtyping import Array, Float\nfrom matplotlib import pyplot as plt\n\nimport klax\n</code></pre> <p>We start by defining a basic class <code>LoadCase</code>. It consists of an array of princible stretches <code>stretch</code> with shape <code>(dim, 3)</code> and the (1,1) component of the first Piola-Kirchhoff stress tensor with shape <code>(dim, 1)</code>, where <code>dim</code> denotes the number of data points in the load case. We also define a descriptive <code>title</code>, which will be used for plotting later on.</p> <pre><code>@dataclass\nclass LoadCase:\n    \"\"\"A simple load case.\"\"\"\n\n    stretch: Float[Array, \"dim 3\"]\n    pk11_stress: Float[Array, \"dim 1\"]\n    title: str\n</code></pre> <p>In the following, three load cases for uniaxial tension, biaxial tension, and pure shear are created. The data corresponds to Treloar`s experimental results, which were obtained from testing of a rubber material.</p> <pre><code># Uniaxial load case\nstretch_1 = jnp.array(\n    [\n        1.0,\n        1.01,\n        1.12,\n        1.24,\n        1.39,\n        1.61,\n        1.89,\n        2.17,\n        2.42,\n        3.01,\n        3.58,\n        4.03,\n        4.76,\n        5.36,\n        5.76,\n        6.16,\n        6.4,\n        6.62,\n        6.87,\n        7.05,\n        7.16,\n        7.27,\n        7.43,\n        7.5,\n        7.61,\n    ]\n)\npk11_stress = jnp.array(\n    [\n        0.0,\n        0.03,\n        0.14,\n        0.23,\n        0.32,\n        0.41,\n        0.5,\n        0.58,\n        0.67,\n        0.85,\n        1.04,\n        1.21,\n        1.58,\n        1.94,\n        2.29,\n        2.67,\n        3.02,\n        3.39,\n        3.75,\n        4.12,\n        4.47,\n        4.85,\n        5.21,\n        5.57,\n        6.3,\n    ]\n)\nstretch = jnp.stack(\n    [stretch_1, stretch_1 ** (-0.5), stretch_1 ** (-0.5)], axis=-1\n)\n\nuniaxial = LoadCase(stretch, pk11_stress, \"Uniaxial tension\")\n\n# Biaxial load case\nstretch_1 = jnp.array(\n    [\n        1.0,\n        1.04,\n        1.08,\n        1.12,\n        1.14,\n        1.2,\n        1.31,\n        1.42,\n        1.69,\n        1.94,\n        2.49,\n        3.03,\n        3.43,\n        3.75,\n        4.03,\n        4.26,\n        4.44,\n    ]\n)\npk11_stress = jnp.array(\n    [\n        0.0,\n        0.09,\n        0.16,\n        0.24,\n        0.26,\n        0.33,\n        0.44,\n        0.51,\n        0.65,\n        0.77,\n        0.96,\n        1.24,\n        1.45,\n        1.72,\n        1.96,\n        2.22,\n        2.43,\n    ]\n)\nstretch = jnp.stack([stretch_1, stretch_1, stretch_1 ** (-2)], axis=-1)\n\nbiaxial = LoadCase(stretch, pk11_stress, \"Biaxial tension\")\n\n# Pure shear load case\nstretch_1 = jnp.array(\n    [\n        1.0,\n        1.06,\n        1.14,\n        1.21,\n        1.32,\n        1.46,\n        1.87,\n        2.4,\n        2.98,\n        3.48,\n        3.96,\n        4.36,\n        4.69,\n        4.96,\n    ]\n)\npk11_stress = jnp.array(\n    [\n        0.0,\n        0.07,\n        0.16,\n        0.24,\n        0.33,\n        0.42,\n        0.59,\n        0.76,\n        0.93,\n        1.11,\n        1.28,\n        1.46,\n        1.62,\n        1.79,\n    ]\n)\nstretch = jnp.stack(\n    [stretch_1, jnp.ones_like(stretch_1), stretch_1 ** (-1)], axis=-1\n)\n\npure_shear = LoadCase(stretch, pk11_stress, \"Pure shear\")\n</code></pre> <p>Next, the neural network-based isotropic, incompressible hyperelastic material model according to Damma\u00df et al. is implemented.</p> <pre><code>class PANN(eqx.Module):\n    \"\"\"An neural-network based potential for incompressible hyperelasticity.\"\"\"\n\n    icnn: klax.nn.FICNN\n\n    def __init__(self, *, key):\n        self.icnn = klax.nn.FICNN(\n            2,\n            \"scalar\",\n            1 * [8],\n            use_passthrough=True,\n            non_decreasing=True,\n            key=key,\n        )\n\n    def __call__(self, stretch: Array) -&gt; Array:\n        \"\"\"Evaluate the (1,1) compontent of the first Piola-Kirchhoff stress.\"\"\"\n        stress_coefficients = self.stress_coefficients(stretch)\n        return self.piola_kirchhoff_stress(stretch, stress_coefficients)\n\n    def potential(self, isochoric_invariants: Array) -&gt; Array:\n        \"\"\"Compute the hyperelastic potential with energy normalization.\"\"\"\n        # Compute normalization term for F = I\n        isochoric_invariants_iden = jnp.array([3, 4])\n        pot_iden = self.icnn(isochoric_invariants_iden)\n        return self.icnn(isochoric_invariants) - pot_iden\n\n    def isochoric_invariants(self, stretch: Array) -&gt; Array:\n        \"\"\"Compute the isotropic, isochoric invariants.\"\"\"\n        i1 = stretch[0] ** 2 + stretch[1] ** 2 + stretch[2] ** 2\n        i2 = (\n            (stretch[0] * stretch[1]) ** 2\n            + (stretch[0] * stretch[2]) ** 2\n            + (stretch[1] * stretch[2]) ** 2\n        )\n        j = stretch[0] * stretch[1] * stretch[2]\n\n        i1_ = j ** (-2 / 3) * i1\n        i2_ = j ** (-4 / 3) * i2\n        return jnp.array([i1_, i2_])\n\n    def stress_coefficients(self, stretch: Array) -&gt; Array:\n        \"\"\"Compute the stress coefficients from the potential.\n\n        They correspond to the gradient of the potential with respect to the\n        isochoric invariants.\n        \"\"\"\n        isochoric_invariants = self.isochoric_invariants(stretch)\n        return jax.grad(self.potential)(isochoric_invariants)\n\n    def piola_kirchhoff_stress(\n        self, stretch: Array, stress_coefficients: Array\n    ):\n        \"\"\"Compute the (1,1) component of the Piola-Kirchoff stress.\"\"\"\n        return 2.0 * (\n            stress_coefficients[0]\n            * (stretch[0] - stretch[2] ** 2 * stretch[0] ** (-1))\n            + stress_coefficients[1]\n            * (\n                stretch[0] * (stretch[1] ** 2 + stretch[2] ** 2)\n                - stretch[2] ** 2\n                * stretch[0] ** (-1)\n                * (stretch[0] ** 2 + stretch[1] ** 2)\n            )\n        )\n</code></pre> <p>We choose the uniaxial and biaxial load cases as calibration data and fit the <code>PANN</code> model using <code>fit()</code> with the following arguments:</p> <p>The first argument corresponds to the model that shall be optimized. Its followed by the <code>data</code> argument, which could be any PyTree containing at least one Array. However, to use <code>klax</code>'s building loss functions the arguments must be of the form <code>(x, y)</code>, where both <code>x</code> and <code>y</code> are the input and output Array respectively.</p> <p>Next, we specify the number of samples per batch and the axis along which <code>x</code> and <code>y</code> shall be batched, by setting the <code>batch_size</code> and <code>batch_axis</code> arguments, respectively.</p> <p>A loss function is defined using the <code>loss_fn</code> argument.</p> <p>We can select an optimization algorithm by passing any <code>optax.GradientTransformation</code> object to the <code>optimizer</code> argument. Likewise we can pass a custom <code>Callback</code> to the <code>history</code> argument to record intermediate training results. In this case we are passing the default <code>HistoryCallback</code> with a value of <code>1000</code>, which ensures that loss-values are only computed after every 1000-th step, to reduce computational cost.</p> <p>Finally, a random <code>key</code> needs to be passed, which is used internally by the batch generation algorithm.</p> <p>If required, <code>fit()</code> delivers many more advanded capabilities such as passing validation data, defining custom batch axes/loss functions/batch generators as well as the use of <code>Callbacks</code> and optimizer states. See the documentation of <code>fit()</code> reference for the full list of options.</p> <p>After finishing calibration, the loss evluation can be plotted by calling the <code>HistoryCallback.plot()</code> method on our returne <code>history</code> object.</p> <pre><code>key = jrandom.PRNGKey(0)\nkeys = jrandom.split(key, 2)\n\nmodel = PANN(key=keys[0])\nx = jnp.concatenate((uniaxial.stretch, biaxial.stretch), axis=0)\ny = jnp.concatenate((uniaxial.pk11_stress, biaxial.pk11_stress), axis=0)\n\nmodel, history = klax.fit(\n    model,\n    (x, y),\n    batch_size=32,\n    batch_axis=0,\n    steps=20_000,\n    loss_fn=klax.mse,\n    optimizer=optax.adam(2e-3),\n    history=klax.HistoryCallback(1000),\n    key=keys[1],\n)\n\nhistory.plot()\n</code></pre> <pre><code>Step: 0, Loss: 1.805e+02\nStep: 1000, Loss: 1.477e-01\nStep: 2000, Loss: 4.639e-02\nStep: 3000, Loss: 2.253e-02\nStep: 4000, Loss: 1.403e-02\nStep: 5000, Loss: 1.052e-02\nStep: 6000, Loss: 8.006e-03\nStep: 7000, Loss: 7.186e-03\nStep: 8000, Loss: 7.112e-03\nStep: 9000, Loss: 6.341e-03\nStep: 10000, Loss: 5.949e-03\nStep: 11000, Loss: 6.724e-03\nStep: 12000, Loss: 5.951e-03\nStep: 13000, Loss: 6.671e-03\nStep: 14000, Loss: 5.807e-03\nStep: 15000, Loss: 6.013e-03\nStep: 16000, Loss: 6.004e-03\nStep: 17000, Loss: 5.959e-03\nStep: 18000, Loss: 5.910e-03\nStep: 19000, Loss: 6.117e-03\nStep: 20000, Loss: 5.688e-03\nTraining took: 0:00:17.094651\n</code></pre> <p></p> <p>A simple utility function is defined to <code>evaluate</code> the calibared <code>model</code> on a tuple of load cases. Here, the model is jit-compiled using <code>eqx.filter_jit()</code> in order to speed up evaluation for large amounts of data.</p> <pre><code>def evaluate(model, load_cases: tuple[LoadCase, ...]):\n    model = eqx.filter_jit(model)\n    _, axs = plt.subplots(1, len(load_cases), figsize=(5 * len(load_cases), 5))\n\n    def plot(ax, x, y, y_pred, title):\n        ax.plot(x, y, \"x\", label=\"data\", color=\"black\")\n        ax.plot(x, y_pred, \"-\", label=\"data\", color=\"blue\")\n        ax.set_xlabel(r\"$\\lambda_1$\")\n        ax.set_ylabel(r\"$P_{11}$/ MPa\")\n        ax.set_title(title)\n\n    for ax, lc in zip(axs, load_cases):\n        pk11_stress_pred = jax.vmap(model)(lc.stretch)\n        plot(ax, lc.stretch[:, 0], lc.pk11_stress, pk11_stress_pred, lc.title)\n</code></pre> <p>We evaluate the model on the two training cases (uniaxial, biaxial) and on the pure shear test case. Note, that the model needs to be made callable using <code>klax.finalize</code>, since the <code>FICNN</code> module contains parameters of type <code>Unwrappable</code>, which need to be unwrapped before evaluation.</p> <pre><code>model_ = klax.finalize(model)\nevaluate(model_, (uniaxial, biaxial, pure_shear))\n</code></pre> <p></p> <p>Finally, the model parameters can be saved using Equinox's <code>equinox.tree_serialize_leaves</code> function. By default it exports to JAX's or Numpy's binary file format. However, to apply the trained model in a finite element simulation (FEM), we usually need to export the parameters in a more universal format.</p> <p>For this, the <code>klax.text_serialize_filter_spec</code> and <code>klax.text_deserialize_filter_spec</code> filter specifications are used, which enable exporting and reading parameters from text files.</p> <pre><code># Save the model parameters to a text file\neqx.tree_serialise_leaves(\n    \"model.txt\", model, filter_spec=klax.text_serialize_filter_spec\n)\n\n# Load the model parameters from a text file\nloaded_model = eqx.tree_deserialise_leaves(\n    \"model.txt\", model, filter_spec=klax.text_deserialize_filter_spec\n)\n\neqx.tree_pprint(loaded_model)\n</code></pre> <pre><code>PANN(\n  icnn=FICNN(\n    layers=(\n      Linear(\n        weight=NonNegative(parameter=f32[2,8]),\n        bias=f32[8],\n        in_features=2,\n        out_features=8,\n        use_bias=True\n      ),\n      InputSplitLinear(\n        weights=(\n          NonNegative(parameter=f32[8,1]), NonNegative(parameter=f32[2,1])\n        ),\n        bias=f32[1],\n        in_features=(8, 2),\n        out_features='scalar',\n        use_bias=True,\n        _num_inputs=2\n      )\n    ),\n    activations=(&lt;PjitFunction of &lt;function softplus at 0x000001D34E9CB1A0&gt;&gt;,),\n    final_activation=&lt;function FICNN.&lt;lambda&gt;&gt;,\n    use_bias=True,\n    use_final_bias=True,\n    use_passthrough=True,\n    non_decreasing=True,\n    in_size=2,\n    out_size='scalar',\n    width_sizes=(8,)\n  )\n)\n</code></pre>"},{"location":"examples/training_without_data/","title":"Training without Data","text":"<p>This example demonstrates one possible way of training a PINNs-style model without any data using <code>klax.fit</code>.</p> <p>To run it locally install klax with plotting capability via <code>pip install 'klax[plot]'</code>.</p> <p>We start by importing the required packages for model creation, optimization and plotting.</p> <pre><code>import equinox as eqx\nimport jax\nimport jax.numpy as jnp\nimport jax.random as jr\nimport optax\nfrom jaxtyping import Array, PRNGKeyArray\nfrom matplotlib import pyplot as plt\n\nimport klax\n</code></pre> <p>Here we want to find the solution \\(u:\\mathbb{R}\\rightarrow\\mathbb{R}\\) of the ODE: $$ \\begin{equation}     \\frac{\\partial u}{\\partial x} + u = 0, \\quad u(x=0)=1 \\end{equation} $$</p> <p>The following code cell implements a PINN model, that uses a small <code>klax.nn.MLP</code> to represent the solution. We also tie the residual loss from the ODE to the PINN implementation as a static method. The PINN instance will store the residual evaluation points \\(x_i\\) in the attribute <code>x_collocation</code>.</p> <p>Note</p> <p>Fore more complex PINN training, writing a custom fit method with random sampling of collocation points, gradient-based loss balancing and other optimization methods may be preferred. For more information see e.g. Wang et al. (2023) \"An Expert's Guide to Training Physics-informed Neural Network\". Meanwhile, the goal of this example is simply to show how training without data could be implemented using Klax.</p> <pre><code>class PINN(eqx.Module):\n    \"\"\"A simple PINN-style model that solves the ODE u_x + u = 0 with u(0) = 1.\"\"\"\n\n    mlp: klax.nn.MLP\n    x_collocation: Array  # Residual evaluation points\n\n    def __init__(self, x_collocation: Array, key: PRNGKeyArray):\n        self.x_collocation = x_collocation\n        self.mlp = klax.nn.MLP(\n            \"scalar\", \"scalar\", 2 * [16], activation=jax.nn.softplus, key=key\n        )\n\n    def __call__(self, x):\n        return self.mlp(x)\n\n    @staticmethod\n    def residural_loss(model, batch, batch_axis):\n        \"\"\"Residual loss definition.\n\n        We define a loss function that penalizes the residual of the ODE\n        u_x + u = 0 and violations of the boundary conditions u(0) = 1.\n        \"\"\"\n        # Manually exclude x_collocation from optimization\n        x_collocation = jax.lax.stop_gradient(model.x_collocation)\n\n        # ODE residual\n        u = jax.vmap(model)(x_collocation)\n        u_xx = jax.vmap(jax.grad(model))(x_collocation)\n        residual = u_xx + u\n        residual_loss = jnp.mean(residual**2)\n\n        # Boundary conditions\n        bc_0 = model(jnp.array(0.0)) - 1.0\n        bc_loss = bc_0**2\n\n        return residual_loss + 0.1 * bc_loss  # Manual loss balancing\n</code></pre> <p>Lets train the model. First we provide the residual evaluation points to initialize the PINN model. Here we choose \\(x_i \\in [0,1]\\). Then we fit the model. Note that we use <code>data=None</code> as the evaluation points are stored within the model itself. Importantly, we also have to specify that our data does not have any <code>batch_axis</code>.</p> <pre><code>x_collocation = jnp.linspace(0, 1, 100)\nmodel_key, training_key = jr.split(jr.key(0))\nmodel = PINN(x_collocation, key=model_key)\n\nmodel, history = klax.fit(\n    model,\n    data=None,\n    batch_axis=None,\n    steps=100_000,\n    optimizer=optax.adam(1e-5),\n    loss_fn=model.residural_loss,\n    history=klax.HistoryCallback(log_every=1000, verbose=False),\n    key=training_key,\n)\n\nhistory.plot()\n</code></pre> <p></p> <p>Finally, lets have a look at the model performance on the interval \\([0,4]\\) and compare it to the analytically calculalted solution \\(u(x)=e^{-x}\\).</p> <pre><code>def solution(x):\n    \"\"\"Return the true solution of the ODE u_x + u = 0 with u(0) = 1.\"\"\"\n    return jnp.exp(-x)\n\n\nx = jnp.linspace(0, 4, 1000)\nu_true = jax.vmap(solution)(x)\nu = jax.vmap(model)(x)\nu_x = jax.vmap(jax.grad(model))(x)\nresidual = u_x + u\n\nfig, axes = plt.subplots(1, 2, figsize=(10, 4))\naxes[0].plot(x, u_true, marker=\"o\", markevery=100, label=R\"$u_{true}(x)$\")\naxes[0].plot(x, u, label=\"PINN\")\naxes[0].set(\n    title=\"PINN solution\",\n    xlabel=\"x\",\n    ylabel=\"u(x)\",\n)\naxes[0].axvspan(\n    model.x_collocation.min(),\n    model.x_collocation.max(),\n    color=\"gray\",\n    alpha=0.2,\n    label=\"Training region\",\n)\naxes[0].legend()\n\naxes[1].plot(x, jnp.abs(residual), label=\"|Residual|\")\naxes[1].set(\n    title=\"Residual of ODE\",\n    xlabel=\"x\",\n    ylabel=\"Absolute value of the residual\",\n    yscale=\"log\",\n)\naxes[1].axvspan(\n    model.x_collocation.min(),\n    model.x_collocation.max(),\n    color=\"gray\",\n    alpha=0.2,\n    label=\"Training region\",\n)\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n</code></pre> <p></p>"}]}